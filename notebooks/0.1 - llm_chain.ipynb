{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa3e7a7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "import torch\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e991698f",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed4f9732",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gap(BaseModel):\n",
    "    title: str\n",
    "    description: str\n",
    "    impact: str\n",
    "    actions: List[str]\n",
    "\n",
    "class GapAnalysis(BaseModel):\n",
    "    company_name: str\n",
    "    job_title: str\n",
    "    generation_date: str\n",
    "    gaps: List[Gap]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64db06c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.mps.is_available())\n",
    "print(torch.mps.device_count())\n",
    "print(torch.backends.mps.is_built())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc61a99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50879fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights:  16%|‚ñà‚ñå        | 54/339 [00:04<00:23, 12.03it/s, Materializing param=model.layers.4.mlp.up_proj.weight]             \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m hf_pipeline = \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext-generation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mQwen/Qwen2.5-7B-Instruct\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmps\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat16\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/resume-builder-agent/venv/lib/python3.11/site-packages/transformers/pipelines/__init__.py:836\u001b[39m, in \u001b[36mpipeline\u001b[39m\u001b[34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, revision, use_fast, token, device, device_map, dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[39m\n\u001b[32m    834\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    835\u001b[39m     model_classes = targeted_task[\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m836\u001b[39m     model = \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    837\u001b[39m \u001b[43m        \u001b[49m\u001b[43madapter_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43madapter_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    838\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    839\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    840\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    841\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    845\u001b[39m hub_kwargs[\u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m] = model.config._commit_hash\n\u001b[32m    847\u001b[39m \u001b[38;5;66;03m# Check which preprocessing classes the pipeline uses\u001b[39;00m\n\u001b[32m    848\u001b[39m \u001b[38;5;66;03m# None values indicate optional classes that the pipeline can run without, we don't raise errors if loading fails\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/resume-builder-agent/venv/lib/python3.11/site-packages/transformers/pipelines/base.py:232\u001b[39m, in \u001b[36mload_model\u001b[39m\u001b[34m(model, config, model_classes, task, **model_kwargs)\u001b[39m\n\u001b[32m    229\u001b[39m kwargs = model_kwargs.copy()\n\u001b[32m    231\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m     model = \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    233\u001b[39m     \u001b[38;5;66;03m# Stop loading on the first successful load.\u001b[39;00m\n\u001b[32m    234\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/resume-builder-agent/venv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:372\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    370\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    371\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m372\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    376\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    377\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    378\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/resume-builder-agent/venv/lib/python3.11/site-packages/transformers/modeling_utils.py:4109\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4093\u001b[39m \u001b[38;5;66;03m# Finalize model weight initialization\u001b[39;00m\n\u001b[32m   4094\u001b[39m load_config = LoadStateDictConfig(\n\u001b[32m   4095\u001b[39m     pretrained_model_name_or_path=pretrained_model_name_or_path,\n\u001b[32m   4096\u001b[39m     ignore_mismatched_sizes=ignore_mismatched_sizes,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4107\u001b[39m     download_kwargs=download_kwargs,\n\u001b[32m   4108\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m4109\u001b[39m load_info = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4110\u001b[39m load_info = \u001b[38;5;28mcls\u001b[39m._finalize_load_state_dict(model, load_config, load_info)\n\u001b[32m   4111\u001b[39m model.eval()  \u001b[38;5;66;03m# Set model in evaluation mode to deactivate Dropout modules by default\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/resume-builder-agent/venv/lib/python3.11/site-packages/transformers/modeling_utils.py:4231\u001b[39m, in \u001b[36mPreTrainedModel._load_pretrained_model\u001b[39m\u001b[34m(cls, model, state_dict, checkpoint_files, load_config)\u001b[39m\n\u001b[32m   4227\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   4228\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNeither a state dict nor checkpoint files were found.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   4230\u001b[39m missing_keys, unexpected_keys, mismatched_keys, disk_offload_index, conversion_errors = (\n\u001b[32m-> \u001b[39m\u001b[32m4231\u001b[39m     \u001b[43mconvert_and_load_state_dict_in_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4232\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4233\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmerged_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4234\u001b[39m \u001b[43m        \u001b[49m\u001b[43mload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4235\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtp_plan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_tp_plan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4236\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype_plan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype_plan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisk_offload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4239\u001b[39m )\n\u001b[32m   4241\u001b[39m \u001b[38;5;66;03m# finally close all opened file pointers\u001b[39;00m\n\u001b[32m   4242\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m all_pointer:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/resume-builder-agent/venv/lib/python3.11/site-packages/transformers/core_model_loading.py:1217\u001b[39m, in \u001b[36mconvert_and_load_state_dict_in_model\u001b[39m\u001b[34m(model, state_dict, load_config, tp_plan, dtype_plan, disk_offload_index)\u001b[39m\n\u001b[32m   1215\u001b[39m pbar.refresh()\n\u001b[32m   1216\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1217\u001b[39m     realized_value, conversion_errors = \u001b[43mmapping\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1218\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfirst_param_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1219\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1220\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1221\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1222\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmissing_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmissing_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconversion_errors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconversion_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1225\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m target_name, param \u001b[38;5;129;01min\u001b[39;00m realized_value.items():\n\u001b[32m   1226\u001b[39m         param = param[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(param, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m param\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/resume-builder-agent/venv/lib/python3.11/site-packages/transformers/core_model_loading.py:696\u001b[39m, in \u001b[36mWeightRenaming.convert\u001b[39m\u001b[34m(self, layer_name, model, config, hf_quantizer, missing_keys, conversion_errors)\u001b[39m\n\u001b[32m    685\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconvert\u001b[39m(\n\u001b[32m    686\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    687\u001b[39m     layer_name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    694\u001b[39m     \u001b[38;5;66;03m# Collect the tensors here - we use a new dictionary to avoid keeping them in memory in the internal\u001b[39;00m\n\u001b[32m    695\u001b[39m     \u001b[38;5;66;03m# attribute during the whole process\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m696\u001b[39m     collected_tensors = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmaterialize_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    698\u001b[39m     \u001b[38;5;66;03m# Perform renaming op (for a simple WeightRenaming, `self.source_patterns` and `self.target_patterns` can\u001b[39;00m\n\u001b[32m    699\u001b[39m     \u001b[38;5;66;03m# only be of length 1, and are actually the full key names - we also have only 1 single related tensor)\u001b[39;00m\n\u001b[32m    700\u001b[39m     target_key = \u001b[38;5;28mself\u001b[39m.target_patterns[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/resume-builder-agent/venv/lib/python3.11/site-packages/transformers/core_model_loading.py:671\u001b[39m, in \u001b[36mWeightTransform.materialize_tensors\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    669\u001b[39m \u001b[38;5;66;03m# Async loading\u001b[39;00m\n\u001b[32m    670\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors[\u001b[32m0\u001b[39m], Future):\n\u001b[32m--> \u001b[39m\u001b[32m671\u001b[39m     tensors = \u001b[43m[\u001b[49m\u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfuture\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[38;5;66;03m# Sync loading\u001b[39;00m\n\u001b[32m    673\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(tensors[\u001b[32m0\u001b[39m]):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/resume-builder-agent/venv/lib/python3.11/site-packages/transformers/core_model_loading.py:671\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    669\u001b[39m \u001b[38;5;66;03m# Async loading\u001b[39;00m\n\u001b[32m    670\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors[\u001b[32m0\u001b[39m], Future):\n\u001b[32m--> \u001b[39m\u001b[32m671\u001b[39m     tensors = [\u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m tensors]\n\u001b[32m    672\u001b[39m \u001b[38;5;66;03m# Sync loading\u001b[39;00m\n\u001b[32m    673\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(tensors[\u001b[32m0\u001b[39m]):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.11/3.11.14_2/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py:451\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m    449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.__get_result()\n\u001b[32m--> \u001b[39m\u001b[32m451\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_condition\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.11/3.11.14_2/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py:327\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m         \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    328\u001b[39m         gotit = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    329\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "hf_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\",\n",
    "    max_new_tokens=256,\n",
    "    device='mps',\n",
    "    dtype=torch.float16\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=hf_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3f0a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps:0\n"
     ]
    }
   ],
   "source": [
    "print(hf_pipeline.model.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548d9d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = PydanticOutputParser(pydantic_object=GapAnalysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0e8137",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"job_description\", \"profile\"],\n",
    "    template=\n",
    "\"\"\"\n",
    "Voc√™ √© um recrutador t√©cnico.\n",
    "\n",
    "Vaga:\n",
    "{job_description}\n",
    "\n",
    "Perfil do candidato:\n",
    "{profile}\n",
    "\n",
    "Tarefa:\n",
    "Analise a vaga e o perfil do candidato e gere uma an√°lise de gaps retorne em formato JSON.\n",
    "\"\"\",\n",
    "    partial_variables={\n",
    "        \"format_instructions\": parser.get_format_instructions()\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f8c54f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"$defs\": {\"Gap\": {\"properties\": {\"title\": {\"title\": \"Title\", \"type\": \"string\"}, \"description\": {\"title\": \"Description\", \"type\": \"string\"}, \"impact\": {\"title\": \"Impact\", \"type\": \"string\"}, \"actions\": {\"items\": {\"type\": \"string\"}, \"title\": \"Actions\", \"type\": \"array\"}}, \"required\": [\"title\", \"description\", \"impact\", \"actions\"], \"title\": \"Gap\", \"type\": \"object\"}}, \"properties\": {\"company_name\": {\"title\": \"Company Name\", \"type\": \"string\"}, \"job_title\": {\"title\": \"Job Title\", \"type\": \"string\"}, \"generation_date\": {\"title\": \"Generation Date\", \"type\": \"string\"}, \"gaps\": {\"items\": {\"$ref\": \"#/$defs/Gap\"}, \"title\": \"Gaps\", \"type\": \"array\"}}, \"required\": [\"company_name\", \"job_title\", \"generation_date\", \"gaps\"]}\\n```'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8227c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2098631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Descri√ß√£o da vaga\\n\\nBuscamos profissional com especialidade em aplicar Intelig√™ncia Artificial Generativa para integrar nossa equipe de projetos voltados ao atendimento ao cliente. Como integrante do time, ser√° respons√°vel por desenvolver e implementar globalmente solu√ß√µes inovadoras utilizando principalmente tecnologias de IA generativa, contribuindo diretamente para otimizar nosso neg√≥cio e transformar a experi√™ncia do cliente atrav√©s de solu√ß√µes tecnol√≥gicas avan√ßadas.\\n\\nResponsabilidades e atribui√ß√µes\\n\\nDesenvolver de ponta a ponta solu√ß√µes de neg√≥cio utilizando tecnologias de IA generativa e aprendizado de m√°quina;\\nUtilizar ferramentas low/no-code como aceleradores do desenvolvimento e implementa√ß√£o de solu√ß√µes;\\nIdentificar e implementar solu√ß√µes t√©cnicas otimizadas para atender requisitos espec√≠ficos de projeto;\\nAnalisar sistematicamente oportunidades de melhoria cont√≠nua em sistemas j√° desenvolvidos;\\nMaximizar a escalabilidade, desempenho, usabilidade e confiabilidade dos aplicativos desenvolvidos;\\nDemonstrar iniciativa de aprendizado aut√¥nomo e atualiza√ß√£o constante sobre novas tecnologias emergentes;\\nParticipar ativamente dos eventos Scrum e garantir comprometimento com cronogramas e entregas acordadas;\\nElaborar documenta√ß√£o t√©cnica detalhada do projeto.\\n\\nRequisitos e qualifica√ß√µes\\n\\nExperi√™ncia profissional comprovada no desenvolvimento de chatbots, agentes de IA e sistemas aut√¥nomos utilizando modelos de linguagem avan√ßados;\\nProfici√™ncia em programa√ß√£o Python e bibliotecas relacionadas a IA/ML;\\nExperi√™ncia s√≥lida no desenvolvimento e implementa√ß√£o de APIs RESTful;\\nProfici√™ncia na linguagem SQL;\\nCompreens√£o dos fundamentos te√≥ricos dos modelos generativos de linguagem e suas aplica√ß√µes pr√°ticas;\\nConhecimento t√©cnico em Engenharia de Prompt;\\nExperi√™ncia com plataformas de computa√ß√£o em nuvem: Azure, Google Cloud ou AWS;\\nForma√ß√£o superior completa;\\nIngl√™s avan√ßado/fluente.\\n\\nDiferenciais:\\n\\nExperi√™ncia pr√°tica com Microsoft Power Platform, especialmente Copilot Studio, Power Apps e Power Automate;\\nDesenvolvimento de agentes de voz utilizando tecnologias de IA generativa;\\nExperi√™ncia em fine-tuning e contextualiza√ß√£o de modelos de linguagem para casos de uso espec√≠ficos;\\nFamiliaridade com Databricks e com ecossistema Azure (AI Foundry, AI services, Speech services, AI search e ML);\\nExperi√™ncia profissional no setor de Consumer Packaged Goods (CPG);\\nJ√° ter trabalhado sob a metodologia Agile e framework Scrum.\\n\\nInforma√ß√µes adicionais\\n\\nVale refei√ß√£o ou alimenta√ß√£o ifood;\\nVale transporte;\\nB√¥nus;\\nPlano de sa√∫de e Telemedicina;\\nConv√™nio odontol√≥gico;\\nConv√™nio Farm√°cia;\\nSeguro de vida;\\nInstituto Ambev de Previd√™ncia Privada;\\nAux√≠lio Material Escolar;\\nCesta de Natal e Kit Congelado;\\nBrinquedos de Natal;\\nEmpresa cidad√£ (licen√ßa maternidade e paternidade) e presente do beb√™;\\nDesconto mensal em produtos AMBEV;\\nBanco de horas;\\nWellhub (Gympass);\\nFretado;\\nAbono anual referente a um sal√°rio.\\n\\nOL√Å, SOMOS A AMBEV TECH! üíõüçªüíª\\n\\nIsso mesmo, o Hub de Tecnologia da Ambev!\\n\\nSomos respons√°veis por impulsionar a transforma√ß√£o digital da Ambev. O match perfeito entre o portf√≥lio da maior cervejaria do mundo e solu√ß√µes de Data & Analytics, Ciberseguran√ßa, Infraestrutura, Suporte, Field, Engenharia e Arquitetura de Software. Contamos com cerca de 50 Squads, que atuam em mais de 100 produtos, e cada linha de c√≥digo criada aqui conecta nossa paix√£o por tecnologia aos mais diversos brindes e suas raz√µes.\\n\\nDo campo ao copo, entregamos solu√ß√µes simples e escal√°veis usando tecnologia de ponta para transformar o dia a dia de milhares de pessoas. E isso s√≥ √© poss√≠vel gra√ßas a um time incr√≠vel que faz acontecer, com inova√ß√£o, senso de dono e muito orgulho de ser tech.\\n\\nPROTAGONIZAMOS SOLU√á√ïES GLOBAIS\\n\\nMuitas das nossas solu√ß√µes s√£o escaladas para a Ambev Global Tech, hub de tecnologia da Anheuser-Busch InBev (AB InBev), multinacional de bebidas e cervejas, e para outras unidades de neg√≥cio da companhia. Com escrit√≥rios espalhados por pa√≠ses como Estados Unidos, M√©xico, √çndia, Argentina e Brasil, estamos conectados a uma rede global de inova√ß√£o e tecnologia para transformar a Ambev em uma plataforma que conecta pessoas e o ecossistema.\\n\\nNOSSO JEITO TECH DE SER\\n\\nPor aqui, sonhamos grande e sabemos a import√¢ncia de ter equipes cada vez mais diversas. √â atrav√©s da pluralidade de ideias, experi√™ncias e opini√µes que conseguimos pensar fora da caixa e criar solu√ß√µes inovadoras. Por isso, contamos com nosso Comit√™ de Autenticidade Tech para olhar com muito carinho e responsabilidade todas as quest√µes relacionadas √† inclus√£o.\\n\\nQUER TRABALHAR COM A GENTE?\\n\\nPara atuar na Ambev Tech com projetos nacionais, temos unidades em Blumenau (SC), Maring√° (PR), S√£o Paulo (SP) e Jaguari√∫na (SP), e contamos tamb√©m com um time remoto em v√°rios cantos do Brasil e Am√©rica Latina. Atuamos com os modelos de trabalho remoto, h√≠brido e presencial, que ir√£o depender da especifica√ß√£o da vaga e de sua localidade, combinado?\\n\\nPara projetos internacionais, na Ambev Global Tech, a contrata√ß√£o acontece no modelo presencial, em Campinas (SP).\\n\\nQuer criar solu√ß√µes que v√£o do campo ao copo, do c√≥digo √† mesa do bar? Vem com a gente e confira todas as nossas oportunidades!\\n\\nWe‚Äôre here for tech and beer.\\n\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./01_jobs/job_1.txt', 'r') as f:\n",
    "\n",
    "    job_description = f.read()\n",
    "\n",
    "job_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65653d0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Felipe Castro\\nData Scientist | Machine Learning | NLP | Applied AI\\nS√£o Paulo ‚Äì SP | (11) 93938-8082 | felmateos@alumni.usp.br | linkedin.com/in/felmateos | github.com/felmateos | kaggle.com/felmateos\\nEXPERI√äNCIA PROFISSIONAL\\nIta√∫ Unibanco 2025 ‚Äì 2026\\nAnalista de Dados S√£o Paulo, SP\\n‚Ä¢ Desenvolvimento de modelo de Reconhecimento de Entidades Nomeadas (NER) com aprendizado supervisionado e fine-tuning de LLMs.\\n‚Ä¢ Aplica√ß√£o em textos corporativos para identifica√ß√£o autom√°tica de produtos e volumes, aumentando a rastreabilidade de ofertas.\\n‚Ä¢ Uso de Python, Transformers e PyTorch em ambiente produtivo.\\nIta√∫ Unibanco 2024 ‚Äì 2025\\nEstagi√°rio de Ci√™ncia de Dados S√£o Paulo, SP\\n‚Ä¢ Sistema de OCR com extra√ß√£o autom√°tica de entidades e valores em documentos estruturados e semiestruturados.\\n‚Ä¢ Uso de IA Generativa para substituir processos manuais, reduzindo tempo de processamento e aumentando efici√™ncia operacional.\\nUSP ‚Äì C4AI / IBM / FAPESP 2023 ‚Äì 2024\\nPesquisador Bolsista em Processamento de Linguagem Natural S√£o Paulo, SP\\n‚Ä¢ Inicia√ß√£o Cient√≠fica em classifica√ß√£o morfossint√°tica de textos sobre vacina√ß√£o contra COVID-19.\\n‚Ä¢ EDA, limpeza, rotula√ß√£o com ferramentas de PLN e an√°lise de distribui√ß√£o dos dados.\\nFORMA√á√ÉO ACAD√äMICA\\nUniversidade de S√£o Paulo (USP) S√£o Paulo, SP\\nMestrado em Sistemas de Informa√ß√£o in√≠cio previsto Mar 2026\\nUniversidade de S√£o Paulo (USP) S√£o Paulo, SP\\nBacharelado em Sistemas de Informa√ß√£o 2020 ‚Äì 2024\\nPROJETOS\\nPredi√ß√£o de Renda com Machine Learning 2025\\nProjeto de Ci√™ncia de Dados ‚Äì Kaggle\\n‚Ä¢ Pipeline de ML estruturado com Kedro para predi√ß√£o de renda anual.\\n‚Ä¢ Valida√ß√£o cruzada estratificada e avalia√ß√£o com F1-score e Precision-Recall para dados desbalanceados.\\n‚Ä¢ An√°lises estat√≠sticas e interpreta√ß√£o de vari√°veis para gera√ß√£o de insights socioecon√¥micos.\\n‚Ä¢ GitHub: github.com/felmateos/income-analysis\\nATIVIDADES EXTRACURRICULARES\\nUSP ‚Äì Diret√≥rio Acad√™mico de Sistemas de Informa√ß√£o 2023\\nMonitor de Introdu√ß√£o √† Programa√ß√£o S√£o Paulo, SP\\nHYPE ‚Äì Ci√™ncia de Dados (Entidade Estudantil) 2022 ‚Äì 2024\\nDiretor de Gest√£o de Pessoas S√£o Paulo, SP\\nCERTIFICA√á√ïES\\nCS50P ‚Äì Introduction to Programming with Python ‚Äî Harvard University (2024)\\nSQL for Data Science ‚Äî University of California, Davis (2024)\\nArtificial Intelligence Fundamentals ‚Äî IBM (2023)\\nData Science Foundations ‚Äî IBM (2023)\\nCONHECIMENTOS\\nLinguagens: Python, SQL | Bibliotecas: Pandas, NumPy, scikit-learn, PyTorch, Transformers, TensorFlow\\nFerramentas: AWS, Git, DataBricks, Kedro, QuickSight | Metodologias: Scrum, CRISP-DM | Idiomas: Portugu√™s Nativo, Ingl√™s C1'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./02_source/source.txt', 'r') as f:\n",
    "\n",
    "    source = f.read()\n",
    "\n",
    "source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec73030b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "result = chain.invoke({\n",
    "    \"job_description\": job_description,\n",
    "    \"profile\": source\n",
    "})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b5e91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.text_generation(\n",
    "    prompt,\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a5b5b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
